
    <!doctype html>
<html lang="en">

<head>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BVKM9SY0E2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BVKM9SY0E2');
</script>


  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha512-xh6O/CkQoPOWDdYTDqeRdPCVd1SpvCA9XXcUnZS2FmJNp1coAFzvtCN9BmamE+4aHK8yyUHUSCcJHgXloTyT2A==" crossorigin="anonymous" referrerpolicy="no-referrer" />

  <title>Nikhil Chowdary Gutlapalli | Robotics Engineer</title>
  <link rel="icon" type="image/x-icon" href="assets/emoji_1.png">


  <link href="https://fonts.googleapis.com/css2?family=Merriweather&display=swap" rel="stylesheet">
    <style>
    html {
      scroll-behavior: smooth;
    }

      body {
        margin: 0;
        font-family: 'Merriweather', serif;
        font-size: 15px;
        line-height: 1.6;
        color: #333;
      }

      h1 {
      font-size: 24px;
      font-weight: bold;
      border-bottom: 2px solid #000;
      display: inline-block;
    }
    h2 {
      font-size: 20px;
      font-weight: bold;
      margin-top: 10px;
    }
    .h2_nik {
      font-size: 20px;
      font-weight: bold;
      margin-top: 30px;
    }
    ul {
      margin-top: 0;
      margin-bottom: 10px;
    }
    li {
      margin-bottom: 4px;
    }
    .li_nik {
      margin-bottom: 8px;
      margin-top: 4px;
    }
    .degree {
      font-weight: bold;
    }
    .school {
      font-style: italic;
    }
    .date {
      float: right;
    } 
.header {
  display: flex;
  align-items: center;
  justify-content: center;
  background-color: #eee;
  padding: 1rem;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  z-index: 9999;
}

.header__logo {
  font-weight: bold;
  font-size: 1.5rem;
  margin-right: auto;
}

.header__nav {
  display: flex;
  align-items: center;
}

.header__nav__list {
  list-style: none;
  margin: 0;
  padding: 0;
  display: flex;
  justify-content: center;
  margin-left: auto;
}

.header__nav__item {
  margin-left: 1.5rem;
}

.header__nav__item:first-child {
  margin-left: 0;
}

.header__nav__item a {
  text-decoration: none;
  color: #333;
  font-weight: bold;
}

      .gpa {
        font-weight: bold;
        margin-top: 5px;
      }
      .experience {
      background-color: #f8f8f8;
      padding: 80px 0;
      }

      .experience__item {
        margin-bottom: 30px;
      }

        .experience__item__title {
          margin: 0;
          font-size: 24px;
          font-weight: bold;
        }

        .experience__item__meta {
          margin: 10px 0;
          font-size: 14px;
        }

        .experience__item__meta__company {
          margin-right: 10px;
        }

        .experience__item__meta__location {
          margin-right: 10px;
        }

        .experience__item__meta__date {
          margin-right: 10px;
        }

        .experience__item__responsibilities {
          margin: 0;
          padding: 0 0 0 20px;
        }

        .experience__item__responsibilities li {
          margin-bottom: 10px;
          font-size: 16px;
        }

        .competitions {
          margin-top: 40px;
        }

        .competitions__list {
          list-style: none;
          padding: 0;
          margin: 0;
        }

        .competitions__item {
          margin-bottom: 20px;
        }

        .competitions__title {
          margin-bottom: 10px;
          font-size: 24px;
        }

        .competitions__title_award {
          margin-bottom: 10px;
          font-size: 18px;
        }

        .competitions__description {
          margin-bottom: 10px;
          font-size: 15px;
        }

        .competitions__result_red {
          font-style: italic;
          color: #FF0000;
        }
        .end {
          font-size: 15px;
        }
        
        @media (max-width: 767px) {
			.header {
				display: none;
			}

      </style>
</head>

<body>
    <header class="header">
  <div class="header__logo">Nikhil Chowdary Gutlapalli</div>
  <nav class="header__nav">
    <ul class="header__nav__list">
      <li class="header__nav__item"><a href="#about-me" class="scroll-to">About Me</a></li>
      <li class="header__nav__item"><a href="#experience">Experience</a></li>
      <li class="header__nav__item"><a href="#skills">Skills</a></li>
      <li class="header__nav__item"><a href="#projects">Projects</a></li>
      <li class="header__nav__item"><a href="#publications">Publications</a></li>
      <li class="header__nav__item"><a href="#competitions">Competitions</a></li>
      <li class="header__nav__item"><a href="#awards">Awards</a></li>
    </ul>
  </nav>
</header>


    <div class="container">
        <div class="row" style="margin-top: 5em; ">
            <div class="col-sm-12" style="margin-bottom: 1em;" id="about-me">
            <h3 class="display-4" style="text-align: center;"><span style="font-weight: bold;">Nikhil </span>Chowdary Gutlapalli</h3>
            </div>
            <br>
            <div class="col-md-8" style="">
                
                <p>
                    <span style="font-weight: bold;">Bio:</span> 
                    I am currently pursuing my Master's degree in Robotics at <a href="https://www.northeastern.edu/" target="_blank">Northeastern University</a>, starting from Spring 2023. My research interests lie in the perception of autonomous vehicles using deep learning and computer vision techniques.
                </p>

                <p>
                    Prior to starting my Master's, I worked as a ML Software Engineer at <a href="https://multicorewareinc.com/" target="_blank">Multicoreware Inc</a> in the Autonomous and Automotive business unit. I gained valuable experience in software development for autonomous systems during my time there.
                </p>

                <p>
                    My passion for robotics began during my undergraduate studies at <a href="https://amrita.edu/" target="_blank">Amrita University</a>, where I majored in Electronics and Communications. I spent three years working as a student researcher at <a href="https://www.amrita.edu/center/humanitarian-technology-hut-labs/" target="_blank">Humanitarian Technology Labs (HuT Labs) </a>, where I worked on various robotics projects, published research papers, and participated in several competitions.
                </p>

                <p>
                    I am excited to continue my academic journey in the field of robotics and make meaningful contributions to the development of autonomous systems. In the future, I hope to explore new and innovative approaches to perception-based robotics and help shape the future of this rapidly-evolving field.
                </p>
                
                <p>For any collaborations, feel free to reach out to me!</p>
                <p>
                    <a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Nikhil_Gutlapalli.pdf" target="_blank" style="margin-right: 15px"><i class="fa fa-address-card fa-lg"></i> CV</a>
                    <a href="mailto:g.nikhilchowdaryatp@gmail.com, gutlapalli.n@northeastern.edu" style="margin-right: 15px"><i class="far fa-envelope-open fa-lg"></i> Mail</a>
                    <a href="https://scholar.google.com/citations?hl=en&user=bFMjC58AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" style="margin-right: 15px"><i class="fa-solid fa-book"></i> Scholar</a>
                    <a href="https://github.com/GutlapalliNikhil" target="_blank" style="margin-right: 15px"><i class="fab fa-github fa-lg"></i> Github</a>
                    <a href="https://www.linkedin.com/in/gutlapalli-nikhil-chowdary/" target="_blank" style="margin-right: 15px"><i class="fab fa-linkedin fa-lg"></i> LinkedIn</a>
                </p>
    
            </div>
            <div class="col-md-4" style="">
                <img src="assets/img/nikhil.png" class="img-thumbnail" width="350px" alt="Profile picture">
            </div>
        </div>





        <div class="row" style="margin-top: 3em;">
            <div class="col-sm-12" style="">
                <h1>Education</h1>

                <h2>Master of Science in Robotics</h2>
                <div class="date">January 2023 - Pursuing</div>
                <div class="school">Northeastern University - Boston, USA üá∫üá∏</div>
                <div class="gpa">CGPA: 4.0</div>

                <ul>
                    <li>Concentration: Electronics and Computer Engineering</li>
                    <li>Relevant coursework: Robotics Sensing and Navigation, Mobile Robotics</li>
                </ul>
                
                <div class="date">June 2016 - May 2020</div>
                <h2 class="h2_nik">Bachelor of Technology in Electronics and Communication Engineering</h2>
                <div class="school">Amrita Vishwa Vidyapeetham - Kerala, India üáÆüá≥</div>
                <div class="gpa">CGPA: 7.7</div>
                    <ul>
                    <li>Relevant coursework: Computer Programming, Information Technology Essentials, Computer System Architecture,   Optimization Techniques.</li>
                    <li id="experience">Awards/Honors: Aspiring Minds Motivational Award (AMMA) by the Dept of ECE.</li>
                    </ul>
            </div>
        </div>



        <div class="row" style="margin-top: 3em;">
            <div class="col-sm-12" style="">
                <h1>Experience</h1>

                <h2>ML Software Engineer</h2>
                <div class="date">October 2020 - December 2022</div>
                <div class="school">MulticoreWare Inc, Chennai, India üáÆüá≥</div>
                <ul>
                    <li>Worked on deep learning projects such as computer vision, convolutional neural networks, and optimization techniques with various clients in the US.</li>
                    <li>Preprocessed and analyzed large-scale datasets using Python and PyTorch.</li>
                    <li>Made a breakthrough of AI in radar technology, which can replace the Angle beamforming (signal processing technique) with Neural Networks in a next-generation automotive radar.</li>
                    <li>Worked with a Leading AI Semiconductor vendor on libraries that provide advanced quantization and compression techniques for trained neural network models and on TensorRT SDK.</li>
                </ul>
                
                <h2 class="h2_nik">Student Researcher</h2>
                <div class="date">December 2017 - August 2020</div>
                <div class="school">Humanitarian Technology Labs in Amrita University, Kerala, India üáÆüá≥</div>
                <ul>
                    <li>Worked on the software stack for the robotics, which includes Auto-Navigation, Speech Recognition and Computer Vision. Specifically, focused on designing and developing the Computer Vision interface to assist the robotic arm and also while performing auto-navigation.</li>
                    <li>Implemented Computer Vision algorithms to detect and recognize objects and obstacles in the robot's environment, allowing the robot to navigate safely and efficiently.</li>
                    <li>Configured and implemented mapping, localization, and pose estimation algorithms using sensor fusion techniques for outdoor and indoor environments. Worked on integrating GPS, LIDAR, and IMU data to create accurate maps and determine the robot's location and orientation in real-time.</li>
                    <li>Collaborated with cross-functional teams to deliver high-quality software products, including integrating the robot's software with hardware components such as motors and sensors.</li>
                    <li id="skills">Contributed to research and development of new robotics technologies, including exploring machine learning and AI techniques for object recognition and autonomous decision-making.</li>
                </ul>
            </div>
        </div>

        <div class="row" style="margin-top: 3em;">
            <div class="col-sm-12" style="">
                <h1>Skills</h1>

                <div class="row">
                <div class="col-md-6">
                  <ul class="skills-list">
                    <li class="li_nik">Python</li>
                    <li class="li_nik">C++</li>
                    <li class="li_nik">Deep Learning Techniques</li>
                    <li class="li_nik">Machine Learning</li>
                    <li class="li_nik">Gazebo</li>
                    <li class="li_nik" id="projects">Embedded Systems</li>
                  </ul>
                </div>
                <div class="col-md-6">
                  <ul class="skills-list">
                    <li class="li_nik">Computer Vision</li>
                    <li class="li_nik">SLAM (Simultaneous Localization and Mapping)</li>
                    <li class="li_nik">PyTorch</li>
                    <li class="li_nik">Sensor Fusion</li>
                    <li class="li_nik">TensorFlow</li>
                    <li class="li_nik">ROS (Robot Operating System)</li>
                  </ul>
                </div>
              </div>
            </div>
        </div>







        <div class="row" style="margin-top: 3em;">
                    <div class="col-sm-15" style="">
                        <h1>Projects</h1>

                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/3D_detection.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/Complex-YOLO-ROS-3D-Object-Detection" target="_blank">Complex YOLO-ROS 3D Object-Detection </a><br>The Complex YOLO ROS 3D Object Detection project is an integration of the Complex YOLOv4 package into the ROS (Robot Operating System) platform, aimed at enhancing real-time perception capabilities for robotics applications. Using 3D object detection techniques based on Lidar data, the project enables robots and autonomous systems to accurately detect and localize objects in a 3D environment, crucial for safe navigation, obstacle avoidance, and intelligent decision-making.<br><a href="https://github.com/GutlapalliNikhil/Complex-YOLO-ROS-3D-Object-Detection" target="_blank">Code</a></div> </div> </div>
			    
                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/pointcloud_Classification.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/Pointcloud-Classification-Pytorch" target="_blank">Point Cloud Classification using PyTorch. </a><br>The Point Cloud Classification project implements the PointNet architecture for classifying 3D point clouds. It supports two datasets, ModelNet and ScanObjectNN, and provides functions for dataset preprocessing, model training, and visualization. The project aims to classify point clouds into different categories using deep learning techniques, enabling applications in object recognition, scene understanding, and robotics. The command-line interface allows easy configuration and training of the model.<br><a href="https://github.com/GutlapalliNikhil/Pointcloud-Classification-Pytorch" target="_blank">Code</a></div> </div> </div>

			<div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/image_classification.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/EmotionDetectAI" target="_blank">Real-time Emotion Detection on Human Faces using YOLOv5 and Android Deployment.</a><br>In my object detection project, I utilized the pre-trained YOLOv5 model from the Ultralytics repository to detect and classify emotions on human faces. Using the open-source software "labelme," I annotated images to create labeled datasets for training and validation. During the training phase, I fine-tuned the YOLOv5 model, optimizing its parameters to accurately identify "Happy" or "Neutral" expressions. Achieving high accuracy with the trained model, I then deployed the weights on an Android phone, developing a custom application for real-time emotion detection. This project showcases the practical application of object detection techniques for emotion recognition, with the ability to deploy such a solution on mobile devices.<br><a href="https://github.com/GutlapalliNikhil/EmotionDetectAI" target="_blank">Code</a></div> </div> </div>
			    
                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/image_classification.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/ImageClassification_VIT_TransferLearning" target="_blank">Custom Image Classification using Pretrained Transformer Model (ViT)</a><br>For my custom image classification project, I utilized a pretrained transformer model to achieve accurate results. By fine-tuning the model during the training phase and evaluating its performance during validation, I successfully adapted it to my specific image classification task. The pretrained transformer model's ability to capture intricate features and patterns in images proved highly effective in achieving high classification accuracy.<br><a href="https://github.com/GutlapalliNikhil/ImageClassification_VIT_TransferLearning" target="_blank">Code</a></div> </div> </div>

			<div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/collaborative_SLAM.gif" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Group4_FinalReport-3.pdf" target="_blank">Collaborative SLAM using Multi-Robot System</a><br>Developed a novel approach to collaborative Simultaneous Localization and Mapping (SLAM) by utilizing a multi-robot system. The experiment employed two turtlebot3 robots operating in a simulated gazebo environment, with ROS serving as the platform for seamless integration. Leveraging the gmapping algorithm, we achieved highly precise mapping of the environment. Additionally, we equipped each robot with autonomous exploration capabilities using explore lite. The maps generated by the robots were skillfully merged using the multirobot map merger, resulting in a comprehensive and detailed representation of the environment.<br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Group4_FinalReport-3.pdf" target="_blank">Project Report</a> / <a href="https://github.com/GutlapalliNikhil/Collaborative_SLAM" target="_blank">Code</a> / <a href="https://www.youtube.com/watch?v=Mosx_JfMLIQ" target="_blank">Demo Video</a> / <a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Collaborative_SLAM.pdf" target="_blank">Slides</a></div> </div> </div>

                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/aa.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/MRProjectReport.pdf" target="_blank">Autonomous Disaster Response & Reconnaissance using TurtleBot 3</a><br>Our project focused on using the TurtleBot3 mobile robot platform for disaster response and reconnaissance. We used a range of ROS packages to build a system that included obstacle avoidance, autonomous navigation, mapping, and object detection. The system used a custom-built AprilTag detection system, which allowed the robot to identify and localize AprilTags in real-time. We overcame challenges associated with deploying mobile robots in complex environments by using ROS technology to ensure seamless communication and control. The system's TCP/IP protocol provided reliable and efficient data transmission between the TurtleBot3 and a master PC. Overall, our project demonstrated the potential of mobile robots for disaster response and showcased the effectiveness of using TurtleBot3 and ROS in these scenarios. Our contributions included designing and implementing the system, addressing technical challenges that arose during the project, and showcasing the advantages of using ROS in disaster response and reconnaissance.<br><a href=https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/MRProjectReport.pdf target="_blank">Project Report</a> / <a href="https://github.com/deep-zspace/Autonomous_Disaster_Response_robot" target="_blank">Code</a> / <a href="https://www.youtube.com/watch?v=UkpYMUttDLI" target="_blank">Demo Video</a> / <a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Mobile%20robotics.pptm.pdf" target="_blank">Slides</a></div> </div> </div>

                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/21.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/project.pdf" target="_blank">Collaborative Robotics on Navigational Platform</a><br>This project involves the development of an autonomous wheelchair with an integrated voice and robotic arm. The system aims to perform Human-Robot-Interaction and Cooperation, Navigation and Mapping in dynamic environments, Opening a Door, and Exchanging objects with a Companion. To achieve this, the team developed their own library for navigation and studied the wheelchair dynamics combined with a robotic arm. The system is validated using hardware and can be controlled either through speech commands or a mobile app. The team used ROS, Gmapping, Navigation Stack, and AMCL for navigation, a custom planner named "HuT Planner" for path planning, and a Markov model for speech recognition. They also created an app for speech recognition and utilized kinova software for the robotic arm. The project can find applications in various settings, including airports, households, and shopping malls, to assist physically disabled and elderly individuals in their daily activities.<br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/project.pdf" target="_blank">Thesis</a> / <a href="https://github.com/GutlapalliNikhil/Install_Speech_Navigation" target="_blank">Code</a> / <a href="https://www.amrita.edu/project/self-e/" target="_blank">More Info </a></div> </div> </div>

                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/chetak.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><b>CHETAK üêé - The Home Service Robot</b><br>CHETAK is a self-governing home assistance robot designed to support individuals with disabilities and impairments with their daily life activities. It features an advanced and cost-effective infrastructure with Object Vision, Speech Recognition, Autonomous Navigation with Obstacle Avoidance, and a 6 Degree of Freedom Robotic Arm, making it a sophisticated service robot. The robot can differentiate among objects based on class and can anticipate qualities like Gender, Age, and Posture. It can receive commands from a person and perform tasks autonomously, either through manual or autonomous navigation, choosing the shortest path while avoiding dynamic obstacles and navigating through extended waypoints. CHETAK uses the robotic arm with end-effector and vision information to pick and place objects. The whole system is implemented using Robot Operating System(ROS) on Ubuntu platform to integrate individual nodes for complex operations. CHETAK can perform Human-Robot interaction, Object Manipulation, and Gesture recognition, helping the disabled and physically handicapped people by serving drinks, fruits, etc., and visually impaired people by finding objects or people and serving them.<br><a href="https://github.com/GutlapalliNikhil/Install_Speech_Navigation" target="_blank">Code</a> / <a href="https://amrita.edu/project/service-robot" target="_blank">More Info </a> / <a href="https://www.youtube.com/watch?v=1iNSb_AOZIM" target="_blank"> Demo Video - 1</a> / <a href="https://www.youtube.com/watch?v=L7wPotpw4lE" target="_blank" id="publications"> Demo Video - 2</a></div> </div> </div>

            </div>
        </div>








        <div class="row" style="margin-top: 1em;">
            <div class="col-sm-12" style="">
                <h1 style="margin-bottom: 2em">Publications</h1>
                <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/system_arch.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Journal.pdf" target="_blank">Robot Operating System based Autonomous Navigation Platform with Human-Robot Interaction</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>, <a href="https://in.linkedin.com/in/vignesh-s-naick-465747166" target="_blank">Vignesh S. Naick</a>, <a href="https://nl.linkedin.com/in/motherammanaswini/" target="_blank">Manaswini Motheram</a>, <a href="https://in.linkedin.com/in/jahnavi-yannam-122606168" target="_blank">Jahnavi Yannam</a>, <span style="font-weight: bold";>Nikhil Chowdary Gutlapalli</span>, <a href="https://scholar.google.com.sg/citations?hl=en&user=UsA_TbwAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Vinu Sivanantham</a> <br><span style="font-style: italic;">TELKOMNIKA Telecommunication, Computing, Electronics and Control</span>, 2023 <br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Journal.pdf" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#1" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="1"><div class="card card-body"><pre><code>@InProceedings{author = {Rajesh Kannan Megalingam, Vignesh S. Naick, Manaswini Motheram, Jahnavi Yannam, Nikhil Chowdary Gutlapalli, Vinu Sivanantham}, 
            title = {Robot Operating System based Autonomous Navigation Platform with Human-Robot Interaction}, 
            booktitle = {TELKOMNIKA Telecommunication, Computing, Electronics and Control}, 
            year = {2023}, 
        }</pre></code></div></div> </div> </div> </div>


            <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/3.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Integration%20of%20Vision%20based%20Robot%20Manipulation%20using%20ROS%20for%20Assistive%20Applications.pdf" target="_blank">Integration of Vision based Robot Manipulation using ROS for Assistive Applications</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>, <a href="https://in.linkedin.com/in/rohith-raj-rv-87229b148" target="_blank">Rokkam Venu Rohith Raj</a>, <a href="https://www.linkedin.com/in/akhil-tammana" target="_blank">Tammana Akhil</a>, <a href="https://in.linkedin.com/in/akhil-masetti-0874b8167" target="_blank">Akhil Masetti</a>, <span style="font-weight: bold";>Nikhil Chowdary Gutlapalli</span>, <a href="https://in.linkedin.com/in/vignesh-s-naick-465747166" target="_blank">Vignesh S. Naick</a> <br><span style="font-style: italic;">Second International Conference on Inventive Research in Computing Applications (ICIRCA)</span>, 2020 <br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Integration%20of%20Vision%20based%20Robot%20Manipulation%20using%20ROS%20for%20Assistive%20Applications.pdf" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#2" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="2"><div class="card card-body"><pre><code>@INPROCEEDINGS{9183013,author={Megalingam, Rajesh Kannan and Rohith Raj, Rokkam Venu and Akhil, Tammana and Masetti, Akhil and Chowdary, Gutlapalli Nikhil and Naick, Vignesh S},
              booktitle={2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA)}, 
              title={Integration of Vision based Robot Manipulation using ROS for Assistive Applications}, 
              year={2020},
              pages={163-169},
              doi={10.1109/ICIRCA48905.2020.9183013}}
        }</pre></code></div></div> </div> </div> </div>


            <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/44.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Human_Robot_Interaction_on_Navigation_platform_using_Robot_Operating_System.pdf" target="_blank">Human Robot Interaction on Navigation platform using Robot Operating System</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>, <a href="https://nl.linkedin.com/in/motherammanaswini/" target="_blank">Manaswini Motheram</a>, <a href="https://in.linkedin.com/in/jahnavi-yannam-122606168" target="_blank">Jahnavi Yannam</a>, <a href="https://in.linkedin.com/in/vignesh-s-naick-465747166" target="_blank">Vignesh S. Naick</a>, <span style="font-weight: bold";>Nikhil Chowdary Gutlapalli</span> <br><span style="font-style: italic;">Fourth International Conference on Inventive Systems and Control (ICISC)</span>, 2020 <br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Human_Robot_Interaction_on_Navigation_platform_using_Robot_Operating_System.pdf" target="_blank">Paper</a> / <a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/HUMAN%20ROBOT%20INTERACTION%20ON%20NAVIGATION%20PLATFORM%20USING%20ROBOT%20OPERATING%20SYSTEM.pdf" target="_blank">Slides</a> / <button class="btn btn-link" type="button" data-toggle="collapse" data-target="#3" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="3"><div class="card card-body"><pre><code>@INPROCEEDINGS{9171065, author={Megalingam, Rajesh Kannan and Manaswini, Motheram and Yannam, Jahnavi and Naick, Vignesh S and Chowdary, Gutlapalli Nikhil},
              booktitle={2020 Fourth International Conference on Inventive Systems and Control (ICISC)}, 
              title={Human Robot Interaction on Navigation platform using Robot Operating System}, 
              year={2020},
              pages={898-905},
              doi={10.1109/ICISC47916.2020.9171065}}
            }</pre></code></div></div> </div> </div> </div>


            <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/55.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Design_and_Implementation_of_an_Arena_for_Testing_and_Evaluating_Quadcopter.pdf" target="_blank">Design and Implementation of an Arena for Testing and Evaluating Quadcopter</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>, <a href="https://in.linkedin.com/in/rohith-raj-rv-87229b148" target="_blank">Rokkam Venu Rohith Raj</a>, <a href="https://in.linkedin.com/in/akhil-masetti-0874b8167" target="_blank">Akhil Masetti</a>, <a href="https://www.linkedin.com/in/akhil-tammana" target="_blank">Tammana Akhil</a>, <span style="font-weight: bold";>Nikhil Chowdary Gutlapalli</span>, <a href="https://in.linkedin.com/in/vignesh-s-naick-465747166" target="_blank">Vignesh S. Naick</a><br><span style="font-style: italic;">4th International Conference on Computing Communication and Automation (ICCCA)</span>, 2018 <br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Design_and_Implementation_of_an_Arena_for_Testing_and_Evaluating_Quadcopter.pdf" target="_blank" id="competitions">Paper</a> / <button class="btn btn-link" type="button" data-toggle="collapse" data-target="#4" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="4"><div class="card card-body"><pre><code>@INPROCEEDINGS{8777578,  author={Megalingam, Rajesh Kannan and Raj, Rokkam Venu Rohith and Masetti, Akhil and Akhil, Tammana and Chowdary, Gutlapalli Nikhil and Naick, Vignesh S},
              booktitle={2018 4th International Conference on Computing Communication and Automation (ICCCA)}, 
              title={Design and Implementation of an Arena for Testing and Evaluating Quadcopter}, 
              year={2018},
              pages={1-7},
              doi={10.1109/CCAA.2018.8777578}}
            }</pre></code></div></div> </div> </div> </div>

            </div>
        </div>






















        <div class="row" style="margin-top: 1em;">
            <div class="col-sm-12" style="">
                <h1 style="margin-bottom: 1.5em">Competitions</h1>
                      <ul class="competitions__list">
                        <li class="competitions__item">
                          <h3 class="competitions__title">DATATHON - Community Code Competition - Boston 2023 üá∫üá∏</h3>
                          <p class="competitions__description">Participated in the Kaggle Community Code Competition, a data science competition that was organized by Northeastern University to challeng participants to develop algorithms for predicting whether a photograph is likely to be popular and generate a significant number of downloads.identifying and tracking cells in microscope images.</p>
                          <p class="competitions__result_red">Ranked in top 30% of participants</p>
                        </li>

                        <li class="competitions__item">
                          <a href="https://www.amrita.edu/news/amrita-student-team-qualify-indy-autonomous-challenge-round-two/" target="_blank"><h3 class="competitions__title">Indy Autonomous Challenge - Indianapolis 2020 üá∫üá∏</h3></a>
                          <p class="competitions__description">Organized by Energy Systems Network, IAC university teams from around the world compete in a series of challenges to advance technology that can speed the commercialization of fully autonomous vehicles and deployments of advanced driver-assistance systems (ADAS) to increase safety and performance.</p>
                          <p class="competitions__result_red">Qualified for the 2nd Round.</p>
                        </li>


                        <li class="competitions__item">
                          <a href="https://www.amrita.edu/news/amrita-teams-win-225-million-japanese-yen-towards-world-robot-summit-2020/" target="_blank"><h3 class="competitions__title">World Robot Summit - Japan 2020 üáØüáµ</h3></a>
                          <p class="competitions__description">The World Robot Summit (WRS) is Challenge and Expo of its kind to bring together Robot Excellence from around the world. Out Of 119 professional teams that applied from all over the world, We are the only team from India to qualify for the finals and took part in the "Future Convenience Store Challenge". </p>
                          <p class="competitions__result_red">Won 1 million JPY Cash Price.</p>
                        </li>


                        <li class="competitions__item">
                          <a href=https://www.amrita.edu/news/hut-labs-student-teams-participate-robocup-german-open-2019/ target="_blank"><h3 class="competitions__title">RoboCup@Home - Germany 2019 üá©üá™</h3></a>
                          <p class="competitions__description">The RoboCup@Home league aims to develop service and assistive robot technology with high relevance for future personal domestic applications. It is the largest international annual competition for autonomous service robots. A set of benchmark tests is used to evaluate the robots‚Äô abilities and performance in a realistic non-standardized home environment setting.</p>
                          <p class="competitions__result_red" id="awards">Qualified for the 2nd Round.</p>
                        </li>
                      </ul>
            </div>
        </div>










        <div class="row" style="margin-top: 1em;">
            <div class="col-sm-12" style="">
                <h1 style="margin-bottom: 1.5em; margin-top: 2em;">Awards</h1>
                      <ul class="competitions__list">

                        <li class="competitions__item">
                          <h4 class="competitions__title_award">Aspiring Minds Motivational Award 2020 - Dept of ECE, Amrita University.</h4>
                        </li>

                        <li class="competitions__item">
                          <h4 class="competitions__title_award">Outstanding Student Reseacher Award 2019 - HuT Labs.</h4>
                        </li>

                        <li class="competitions__item">
                          <h4 class="competitions__title_award">Certificate of Excellence 2019 - 2020 by IEEE Student Branch.</h4>
                        </li>
                      </ul>
            </div>
        </div>

        <div class="row" style="margin-top: 3em; margin-bottom: 1em;">
            
            <div class="col-sm-12" style="">
                <h4 class="end">Website template provided by <a href=https://m-niemeyer.github.io/ target="_blank">Michael Niemeyer</a>. Check out his <a href=https://github.com/m-niemeyer/m-niemeyer.github.io target="_blank">github repository </a> for instructions on how to use it! <h4>
            </div>
    
        </div>
    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
      integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
      crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"
      integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
      crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"
      integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
      crossorigin="anonymous"></script>
</body>

</html>
    
